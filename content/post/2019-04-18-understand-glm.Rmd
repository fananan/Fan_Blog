---
title: Understand GLM -- Part1 Assumptions
author: Fan Liu
date: '2019-04-18'
slug: understand-glm
categories:
  - GLM
  - R
tags:
  - GLM
  - R
  - regression
data: '2019-04-18T21:49:01-04:00'
description: 'This is series of blogs about me trying to understand GLM inside and out'
image: 'img/rainbow.jpg'
---

This series of posts are inspired by my friends Mehrnaz and Anthi during a coffee chat.

##### Quick Overview of Comparing Defining Features between GLM and Linear regression ([reference](<https://www.theanalysisfactor.com/confusing-statistical-term-7-glm/>)):
* Linear Regression (ANOVA, ANCOVA...)
    1. Residuals of dependent variable are normally distributed
    2. All coefficient estimates, means, residual variance are               calculated by OLS (ordinary least squares). (This feature is          important since some statistics are derived from OLS:                 R-squared, MSE)
* Generalized Linear Model
    1. Residuals of dependent variable are not necessarily normally          distributed. Their distributions are from exponential family
    2. Dependent variable is linear with predictors only through a           link function
    3. All coefficient estimates, means, residual variance are               calculated through MLE, not OLS

##### First and Foremost, be clear about Linear Regression assumptions:
[SML](<https://www.theanalysisfactor.com/the-distribution-of-dependent-variables/>): the assumptions of normality and homogeneity (to validate the statistical properties of any one part of an overall dataset are the same as any other part) are not about Y, the dependent variable, but about residual errors, which represent variation in which is not explained by the predictors.

Assumptions are checked after you creating a model...

* Explicit Assumptions (inferential procedures for linear regression are typically based on a normality distribution for the residuals):
    1. The errors are independent
    2. The errors are normally distributed
    3. The errors have constant variance, which means their variance does not change across different levels of the predictors.
    4. The errors are sumed to zero 
    5. Linearity and additivity of the relationship between dependent and independent variables: the expected value of dependent variable is a straight-line function of EACH independent variable, holding the others fixed; the slope of that line does not depend on the values of other variables; the effects of different independent variables on the expected values of the dependent variables are additive
     (residuals are the estimates of errors)
These five are enough for most linear regression analysis.

! Be aware that as sample size increases, the normality of residuals is not needed. You can use Bootstrap to estimate coefficients; but if the residuals do not satisfying constant variance assuption, standard errors and confidence intervals will be adversely affected, no matter how big the sample size it. 

How to diagnose those assumptions and fix it:
([reference](<http://people.duke.edu/~rnau/testing.htm>))

* "Residuals vs. Fitted: Nonlinearity is usually most evident in a plot of observed versus predicted values or a plot of residuals versus predicted/fitted values. (Solution: nonlinear transformation to the dependent varaible or/and independent variables. See details in the reference.)

* "Normal Q-Q": check for normality.This is the plot of the fractiles of error distribution versus the fractiles of a normal distribution by having the same mean and varaince. 
   
* Violation of independence can be diagnosed by looking at residuals versus independent variables. The residuals should be randomly and symmetrically distributed around zero under all conditions. 

* To check homoscedasticity: plot residuals versus predicted/fittd values

##### Major differences between GLM and LM
* GLM relaxes normality
* GLM allows for non-uniform variance (variance functions)
* GLM doesn't assume direct linear relationship between response variable and predictors
* GLM uses MLE to estimate parameters 


[a practical procedure of building GLM](<https://www.guru99.com/r-generalized-linear-model.html>)

#### The goal of your tech-related blogs should look like this format: <http://r-statistics.co/Assumptions-of-Linear-Regression.html>

##### The first post will take you back to the basic assumptions about linear model and multicollinearity

#### Update on 8/19/2019: too much theoretical contents, you need some examples to illustrate your points...

##### 1. Assumptions of Linear Model:
##### First thing first: why do we need to have assumptions: we don't need them to fit least sqaured model, but violating the assumptions will make yourself more difficult in interpreting the regression results, for example, constructing a confidence interval.
* relationship between the independent and dependent variables should be linear. it is also important to check for outliers since linear regression is sensitive to outlier effect. The linearity assumption can be tested through scatter plots.
    + **Pay Attention**: Y=a+(β1_X1)+(β2_X2^2) is also linear regression model, even X2 is raised to power 2. Linear in "parameters".
* Independence of residuals error term. there is little or no autocorrelation in the data. Autocorrelation occurs when residuals are not independent from each other. 
    + Difference between **statistical error** and **residual**: statistical error is the deviation of observed values from *TRUE* population mean which is unobservable; residual is the deviation of oberved values from *ESTIMATED* sample mean which could serve as a good estimator of the population mean. So the sum of residuals in a random sample is necessarily zero, and thus residuals are necessarily *not independen*; The statistical error, on the other hand, are independent, but their sum in a random sample is almost surely not zero.
    + This distinction is important when it comes to calculate **Mean Sqaured Error**. The MSE is calculated from sum of squared residuals, not unbservable error. When we divide it by n, the number of observations, it's MSE. But this is biased estimate of unobserved error. The bias is removed by dividing sum of squaured residuals by df=n-p-1 (n is the number of observations, p is number of parameters).
* Normality of residuals. requires all variables to be multivariate normal. This assumption can be best checked with Q-Q plot.
Normality can be checked with a goodness of fit test: Kolmogorov-smirnov test. When data is not normally distributed a non-linear transformation (e.g. log, square, cubic) might fix the issue.
(what is Q-Q plot: quantile-quantile, a point on the plot corresponds to one of the quantiles of the second distribution plotted against the same quantile of the first distribution. if the two distibutions being compared similar, the points in Q-Q plot will approximately lie on the line y=x)
    + try to understand Q-Q plot first:If the two distributions being compared are identical, the Q–Q plot follows the 45° line y = x. If the two distributions agree after linearly transforming the values in one of the distributions, then the Q–Q plot follows some line, but not necessarily the line y = x. If the general trend of the Q–Q plot is flatter than the line y = x, the distribution plotted on the horizontal axis is more dispersed than the distribution plotted on the vertical axis. Conversely, if the general trend of the Q–Q plot is steeper than the line y = x, the distribution plotted on the vertical axis is more dispersed than the distribution plotted on the horizontal axis.
        - dispersed: bigger standard deviation/variance, in other words, it's fatter and longer
<!--- ![dispersion](/Users/fanliu/Documents/Fan's Blog/public/img/dispersion.jpg)--> 
        - get sample data here and plot Q-Q plot to understand "S" shaped Q-Q plot in R in R


how to identify multicollinearity? VIF? variance inflation factor
bad effects multicollinearity brings for the results
how to fix it

##### 2. About Multicollinearity:

##### Two things I learned from the website: <https://newonlinecourses.science.psu.edu/stat501/node/346/>:
*  When there are two highly-correlate variables A and B. The coefficients of putting them separately are definitely larger than the ones when you put them two together in the model, because the former one indicates when you increase A, B increases too, which increases the response more; but the latter means, when you fix the value of one of them, and you increase the other,then the response won't increase that much as the first situation. 
*  About ESS: explained sum of squares. "The amazing thing is that even predictors that are not included in the model, but are highly correlated with the predictors in our model, can have an impact! ... The moral of the story is that if you get estimated coefficients that just don't make sense, there is probably a very good explanation. Rather than stopping your research and running off to report your unusual results, think long and hard about what might have caused the results. That is, think about the system you are studying and all of the extraneous variables that could influence the system."
    + Example shown from this website: For example, consider a pharmaceutical company's regression of territory sales on territory population and per capita income. One would, of course, expect that as the population of the territory increases, so would the sales in the territory. But, contrary to this expectation, the pharmaceutical company's regression analysis deemed the estimated coefficient of territory population to be negative. That is, as the population of territory increases, the territory sales were predicted to decrease. After further investigation, the pharmaceutical company determined that the larger the territory, the larger too the competitor's market penetration. That is, the competitor kept the sales down in territories with large populations.
    + **Your own conclusion**: that's why business mindset and domain knowledge is important. Since you could have an idea to explain things that look weird from data results from the first sight
    + it's really helpful to refresh my understanding about RSS, ESS and TSS. TSS = RSS + ESS. Larger ESS, better the model fits. ESS measures how far regression line away from the average line. Intuitively, you can think if ESS too close to average line, then this model is useless, since you could just use average line to "predict" rather than build a regression model. 
        i) Another interesing fact about R squre. R^2 is the ratio of ESS. When it's just simple regression (with only one predictor), R^2 is equal to r^2. r is correlation value between Y and X. But this doesn't work for multilinear regression, since correlation only measure the relationship between two variables. 

#### Update on 10/5/2019: learned from Jun
##### GLM can only use maximum likelihood, but linear regression can use both maximum likelihood and mean square error.
#### The ultimate goal of max likelihood is to find an optimal way to find the distribution of the data. Likelihood is you know the data first and then find its corresponding distribution. Likelihood formula IS distribution formula, it's just to use distribution formula and multiplies together using all data points. How to get maximum likelihood is to get derivative. For example, maximum likelihood of normal distribution, take log first (since it has exponential in it) before taking derivatives. MLE is the short name for maximum likelihood.    

###### if errors are following normal distribution around the mean, then MLE = OLS (OLS -> minimize error -> maximize likelihood -> MLE from the formula)

#### how to derive log likelihood formula 

##### 'variance' (more precisely, RSS: residual sum of square) and deviance 
##### Deviance: This expression is simply 2 times the log-likelihood ratio of the full model compared to the reduced model. The deviance is used to compare two models – in particular in the case of generalized linear models (GLM) where it has a similar role to residual variance from ANOVA in linear models (RSS).